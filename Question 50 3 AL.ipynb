{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nosao\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\nosao\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nosao\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset before mapping: ['Response C' 'Response B' 'Response A' 'Response D']\n",
      "Training Logistic Regression...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression best params: {'clf__C': 10, 'clf__solver': 'lbfgs', 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "Logistic Regression best accuracy: 0.9385507246376813\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best params: {'clf__C': 10, 'clf__kernel': 'linear', 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 1)}\n",
      "SVM best accuracy: 0.9211594202898551\n",
      "Training Random Forest...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest best params: {'clf__n_estimators': 50, 'tfidf__max_df': 1.0, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 1)}\n",
      "Random Forest best accuracy: 0.9206763285024154\n",
      "\n",
      "Best model is Logistic Regression with accuracy of 0.9385507246376813\n",
      "\n",
      "Logistic Regression classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.83      0.77        18\n",
      "           2       0.90      0.87      0.88        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.84        57\n",
      "   macro avg       0.87      0.83      0.84        57\n",
      "weighted avg       0.86      0.84      0.84        57\n",
      "\n",
      "Predicted Response: Response C\n",
      "Predicted Response: Response C\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install nltk pandas scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('labeled_job_descriptions.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['description'] = data['Job description'].apply(preprocess_text)\n",
    "\n",
    "# Print unique labels to check for discrepancies\n",
    "print(\"Unique labels in the dataset before mapping:\", data['Label'].unique())\n",
    "\n",
    "# Update the label mapping to match your dataset\n",
    "label_mapping = {\n",
    "    'Response A': 0,\n",
    "    'Response B': 1,\n",
    "    'Response C': 2,\n",
    "    'Response D': 3,\n",
    "    'Response E': 4,  # If 'Response E' exists\n",
    "    'Response F': 5   # If 'Response F' exists\n",
    "}\n",
    "\n",
    "# Map the labels\n",
    "data['Label'] = data['Label'].map(label_mapping)\n",
    "\n",
    "# Check for any missing values in the Label column after mapping\n",
    "if data['Label'].isnull().any():\n",
    "    print(\"There are missing values in the Label column. Please check the data.\")\n",
    "    # Print rows with missing Labels\n",
    "    print(data[data['Label'].isnull()])\n",
    "else:\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['description'], data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Building pipelines for different classifiers\n",
    "    pipelines = {\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', LogisticRegression(class_weight='balanced'))\n",
    "        ]),\n",
    "        'SVM': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', SVC(class_weight='balanced', probability=True))\n",
    "        ]),\n",
    "        'Random Forest': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', RandomForestClassifier(class_weight='balanced'))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # Define parameter grids for each classifier\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__solver': ['liblinear', 'lbfgs']\n",
    "        },\n",
    "        'SVM': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__n_estimators': [50, 100, 200]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    best_accuracy = 0\n",
    "    best_model_name = None\n",
    "\n",
    "    # Perform GridSearchCV for each model\n",
    "    for model_name in pipelines:\n",
    "        print(f\"Training {model_name}...\")\n",
    "        grid_search = GridSearchCV(pipelines[model_name], param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        accuracy = grid_search.best_score_\n",
    "        print(f\"{model_name} best params: {grid_search.best_params_}\")\n",
    "        print(f\"{model_name} best accuracy: {accuracy}\")\n",
    "\n",
    "        # Check if this model is the best so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = model_name\n",
    "\n",
    "    print(f\"\\nBest model is {best_model_name} with accuracy of {best_accuracy}\")\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    best_model = best_models[best_model_name]\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(f'\\n{best_model_name} classification report on test set:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Function to predict the response for a new job description using the best model\n",
    "    def predict_response(job_description):\n",
    "        processed_description = preprocess_text(job_description)\n",
    "        prediction = best_model.predict([processed_description])[0]\n",
    "        reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "        return reverse_label_mapping[prediction]\n",
    "\n",
    "    # Example usage\n",
    "    new_job_description = \"Manage university financial reports and budget forecasting.\"\n",
    "    predicted_response = predict_response(new_job_description)\n",
    "    print(f'Predicted Response: {predicted_response}')\n",
    "\n",
    "    new_job_description_other = \"Assist in organizing office files and managing schedules.\"\n",
    "    predicted_response_other = predict_response(new_job_description_other)\n",
    "    print(f'Predicted Response: {predicted_response_other}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Response: Response A\n"
     ]
    }
   ],
   "source": [
    "new_job_description = \"The Director of PMO & Strategic Change will lead the delivery of the University’s strategic change portfolio and Programme Management Offices. The Director will work in partnership with senior leaders and key stakeholders across the institution to ensure the delivery of the strategic change programme that underpins delivery of the University’s strategic plan.\"\n",
    "predicted_response = predict_response(new_job_description)\n",
    "print(f'Predicted Response: {predicted_response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nosao\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\nosao\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nosao\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset before mapping: ['Response C' 'Response B' 'Response A' 'Response D']\n",
      "Training Logistic Regression...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression best params: {'clf__C': 10, 'clf__solver': 'lbfgs', 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "Logistic Regression best accuracy: 0.9385507246376813\n",
      "\n",
      "Logistic Regression classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.83      0.77        18\n",
      "           2       0.90      0.87      0.88        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.84        57\n",
      "   macro avg       0.87      0.83      0.84        57\n",
      "weighted avg       0.86      0.84      0.84        57\n",
      "\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best params: {'clf__C': 10, 'clf__kernel': 'linear', 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 1)}\n",
      "SVM best accuracy: 0.9211594202898551\n",
      "\n",
      "SVM classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.89      0.80        18\n",
      "           2       0.93      0.87      0.90        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.86        57\n",
      "   macro avg       0.89      0.84      0.86        57\n",
      "weighted avg       0.88      0.86      0.86        57\n",
      "\n",
      "Training Random Forest...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest best params: {'clf__n_estimators': 200, 'tfidf__max_df': 1.0, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "Random Forest best accuracy: 0.9251207729468598\n",
      "\n",
      "Random Forest classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.94      0.87        18\n",
      "           2       0.96      0.87      0.91        30\n",
      "           3       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.91        57\n",
      "   macro avg       0.92      0.94      0.93        57\n",
      "weighted avg       0.92      0.91      0.91        57\n",
      "\n",
      "\n",
      "Best model is Logistic Regression with accuracy of 0.9385507246376813\n",
      "Predicted Response: Response C\n",
      "Predicted Response: Response C\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install nltk pandas scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('labeled_job_descriptions.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['description'] = data['Job description'].apply(preprocess_text)\n",
    "\n",
    "# Print unique labels to check for discrepancies\n",
    "print(\"Unique labels in the dataset before mapping:\", data['Label'].unique())\n",
    "\n",
    "# Update the label mapping to match your dataset\n",
    "label_mapping = {\n",
    "    'Response A': 0,\n",
    "    'Response B': 1,\n",
    "    'Response C': 2,\n",
    "    'Response D': 3,\n",
    "    'Response E': 4,  # If 'Response E' exists\n",
    "    'Response F': 5   # If 'Response F' exists\n",
    "}\n",
    "\n",
    "# Map the labels\n",
    "data['Label'] = data['Label'].map(label_mapping)\n",
    "\n",
    "# Check for any missing values in the Label column after mapping\n",
    "if data['Label'].isnull().any():\n",
    "    print(\"There are missing values in the Label column. Please check the data.\")\n",
    "    # Print rows with missing Labels\n",
    "    print(data[data['Label'].isnull()])\n",
    "else:\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['description'], data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Building pipelines for different classifiers\n",
    "    pipelines = {\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', LogisticRegression(class_weight='balanced'))\n",
    "        ]),\n",
    "        'SVM': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', SVC(class_weight='balanced', probability=True))\n",
    "        ]),\n",
    "        'Random Forest': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', RandomForestClassifier(class_weight='balanced'))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # Define parameter grids for each classifier\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__solver': ['liblinear', 'lbfgs']\n",
    "        },\n",
    "        'SVM': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__n_estimators': [50, 100, 200]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    best_accuracy = 0\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "\n",
    "    # Perform GridSearchCV for each model\n",
    "    for model_name in pipelines:\n",
    "        print(f\"Training {model_name}...\")\n",
    "        grid_search = GridSearchCV(pipelines[model_name], param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        accuracy = grid_search.best_score_\n",
    "        print(f\"{model_name} best params: {grid_search.best_params_}\")\n",
    "        print(f\"{model_name} best accuracy: {accuracy}\")\n",
    "\n",
    "        # Evaluate each model on the test set\n",
    "        y_pred = best_models[model_name].predict(X_test)\n",
    "        print(f'\\n{model_name} classification report on test set:')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Check if this model is the best so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = model_name\n",
    "            best_model = best_models[model_name]\n",
    "\n",
    "    print(f\"\\nBest model is {best_model_name} with accuracy of {best_accuracy}\")\n",
    "\n",
    "    # Function to predict the response for a new job description using the best model\n",
    "    def predict_response(job_description):\n",
    "        processed_description = preprocess_text(job_description)\n",
    "        prediction = best_model.predict([processed_description])[0]\n",
    "        reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "        return reverse_label_mapping[prediction]\n",
    "\n",
    "    # Example usage\n",
    "    new_job_description = \"Manage university financial reports and budget forecasting.\"\n",
    "    predicted_response = predict_response(new_job_description)\n",
    "    print(f'Predicted Response: {predicted_response}')\n",
    "\n",
    "    new_job_description_other = \"Assist in organizing office files and managing schedules.\"\n",
    "    predicted_response_other = predict_response(new_job_description_other)\n",
    "    print(f'Predicted Response: {predicted_response_other}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Response: Response B\n",
      "Predicted Response: Response B\n"
     ]
    }
   ],
   "source": [
    "new_job_description = \" To continually review and reflect upon the currency and operation of the University’s Academic Regulations, benchmarking against the sector and regulatory requirements as appropriate, and when required to drive forward any changes necessary to the Academic Regulations.\"\n",
    "predicted_response = predict_response(new_job_description)\n",
    "print(f'Predicted Response: {predicted_response}')\n",
    "\n",
    "new_job_description_other = \"To continually review and reflect upon the currency and operation of the University’s Academic Regulations, benchmarking against the sector and regulatory requirements as appropriate, and when required to drive forward any changes necessary to the Academic Regulations.\"\n",
    "predicted_response_other = predict_response(new_job_description_other)\n",
    "print(f'Predicted Response: {predicted_response_other}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nosao\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\nosao\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nosao\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset before mapping: ['Response C' 'Response B' 'Response A' 'Response D']\n",
      "Training Logistic Regression...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression best params: {'clf__C': 10, 'clf__solver': 'lbfgs', 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "Logistic Regression best accuracy: 0.9385507246376813\n",
      "\n",
      "Logistic Regression classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.83      0.77        18\n",
      "           2       0.90      0.87      0.88        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.84        57\n",
      "   macro avg       0.87      0.83      0.84        57\n",
      "weighted avg       0.86      0.84      0.84        57\n",
      "\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best params: {'clf__C': 10, 'clf__kernel': 'linear', 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 1)}\n",
      "SVM best accuracy: 0.9211594202898551\n",
      "\n",
      "SVM classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.89      0.80        18\n",
      "           2       0.93      0.87      0.90        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.86        57\n",
      "   macro avg       0.89      0.84      0.86        57\n",
      "weighted avg       0.88      0.86      0.86        57\n",
      "\n",
      "Training Random Forest...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest best params: {'clf__n_estimators': 100, 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "Random Forest best accuracy: 0.9207729468599034\n",
      "\n",
      "Random Forest classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.89      0.80        18\n",
      "           2       0.93      0.87      0.90        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.86        57\n",
      "   macro avg       0.89      0.84      0.86        57\n",
      "weighted avg       0.88      0.86      0.86        57\n",
      "\n",
      "\n",
      "Best model is Logistic Regression with accuracy of 0.9385507246376813\n",
      "Job Description: Manage university financial reports and budget forecasting.\n",
      "Predicted Response: Response C\n",
      "\n",
      "Job Description: Assist in organizing office files and managing schedules.\n",
      "Predicted Response: Response C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install nltk pandas scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('labeled_job_descriptions.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['description'] = data['Job description'].apply(preprocess_text)\n",
    "\n",
    "# Print unique labels to check for discrepancies\n",
    "print(\"Unique labels in the dataset before mapping:\", data['Label'].unique())\n",
    "\n",
    "# Update the label mapping to match your dataset\n",
    "label_mapping = {\n",
    "    'Response A': 0,\n",
    "    'Response B': 1,\n",
    "    'Response C': 2,\n",
    "    'Response D': 3,\n",
    "    'Response E': 4,  # If 'Response E' exists\n",
    "    'Response F': 5   # If 'Response F' exists\n",
    "}\n",
    "\n",
    "# Map the labels\n",
    "data['Label'] = data['Label'].map(label_mapping)\n",
    "\n",
    "# Check for any missing values in the Label column after mapping\n",
    "if data['Label'].isnull().any():\n",
    "    print(\"There are missing values in the Label column. Please check the data.\")\n",
    "    # Print rows with missing Labels\n",
    "    print(data[data['Label'].isnull()])\n",
    "else:\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['description'], data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Building pipelines for different classifiers\n",
    "    pipelines = {\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', LogisticRegression(class_weight='balanced'))\n",
    "        ]),\n",
    "        'SVM': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', SVC(class_weight='balanced', probability=True))\n",
    "        ]),\n",
    "        'Random Forest': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', RandomForestClassifier(class_weight='balanced'))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # Define parameter grids for each classifier\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__solver': ['liblinear', 'lbfgs']\n",
    "        },\n",
    "        'SVM': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__kernel': ['linear', 'rbf']\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__n_estimators': [50, 100, 200]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    best_accuracy = 0\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "\n",
    "    # Perform GridSearchCV for each model\n",
    "    for model_name in pipelines:\n",
    "        print(f\"Training {model_name}...\")\n",
    "        grid_search = GridSearchCV(pipelines[model_name], param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        accuracy = grid_search.best_score_\n",
    "        print(f\"{model_name} best params: {grid_search.best_params_}\")\n",
    "        print(f\"{model_name} best accuracy: {accuracy}\")\n",
    "\n",
    "        # Evaluate each model on the test set\n",
    "        y_pred = best_models[model_name].predict(X_test)\n",
    "        print(f'\\n{model_name} classification report on test set:')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Check if this model is the best so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = model_name\n",
    "            best_model = best_models[model_name]\n",
    "\n",
    "    print(f\"\\nBest model is {best_model_name} with accuracy of {best_accuracy}\")\n",
    "\n",
    "    # Function to predict the response for multiple job descriptions using the best model\n",
    "    def predict_responses(job_descriptions):\n",
    "        processed_descriptions = [preprocess_text(desc) for desc in job_descriptions]\n",
    "        predictions = best_model.predict(processed_descriptions)\n",
    "        reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "        return [reverse_label_mapping[prediction] for prediction in predictions]\n",
    "\n",
    "    # Example usage\n",
    "    job_descriptions = [\n",
    "        \"Manage university financial reports and budget forecasting.\",\n",
    "        \"Assist in organizing office files and managing schedules.\"\n",
    "    ]\n",
    "    predicted_responses = predict_responses(job_descriptions)\n",
    "    for job_desc, response in zip(job_descriptions, predicted_responses):\n",
    "        print(f'Job Description: {job_desc}\\nPredicted Response: {response}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Manage university financial reports and budget forecasting.\n",
      "Predicted Response: Response C\n",
      "\n",
      "Job Description: Assist in organizing office files and managing schedules.\n",
      "Predicted Response: Response C\n",
      "\n",
      "Job Description: To plan, manage and deliver student recruitment events based on-campus and virtually..\n",
      "Predicted Response: Response C\n",
      "\n",
      "Job Description: \tTo monitor, review and report on the impact and effectiveness of all student recruitment events…\n",
      "Predicted Response: Response B\n",
      "\n",
      "Job Description: The Director of PMO & Strategic Change will lead the delivery of the University’s strategic change portfolio and Programme Management Offices. The Director will work in partnership with senior leaders and key stakeholders across the institution to ensure the delivery of the strategic change programme that underpins delivery of the University’s strategic plan.\n",
      "Predicted Response: Response A\n",
      "\n",
      "Job Description: To contribute to continuing improvements to management and financial systems and to the maintenance of effective administration.\n",
      "Predicted Response: Response D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Function to predict the response for multiple job descriptions using the best model\n",
    "def predict_responses(job_descriptions):\n",
    "  processed_descriptions = [preprocess_text(desc) for desc in job_descriptions]\n",
    "  predictions = best_model.predict(processed_descriptions)\n",
    "  reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "  return [reverse_label_mapping[prediction] for prediction in predictions]  \n",
    "\n",
    "# Example usage\n",
    "job_descriptions = [\n",
    "\"Manage university financial reports and budget forecasting.\",\n",
    "\"Assist in organizing office files and managing schedules.\",\n",
    "\"To plan, manage and deliver student recruitment events based on-campus and virtually..\",\n",
    "\"\tTo monitor, review and report on the impact and effectiveness of all student recruitment events…\",\n",
    "\"The Director of PMO & Strategic Change will lead the delivery of the University’s strategic change portfolio and Programme Management Offices. The Director will work in partnership with senior leaders and key stakeholders across the institution to ensure the delivery of the strategic change programme that underpins delivery of the University’s strategic plan.\",\n",
    "\"To contribute to continuing improvements to management and financial systems and to the maintenance of effective administration.\"\n",
    "\n",
    "]\n",
    "predicted_responses = predict_responses(job_descriptions)\n",
    "for job_desc, response in zip(job_descriptions, predicted_responses):\n",
    "  print(f'Job Description: {job_desc}\\nPredicted Response: {response}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: To provide of an effective Joinery resource to ensure the University fabric is efficiently maintained on a day-to-day basis including undertaking Project works. To ensure the effective interaction of Estate and Facilities services with other services\n",
      "Predicted Response: Response B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Function to predict the response for multiple job descriptions using the best model\n",
    "def predict_responses(job_descriptions):\n",
    "  processed_descriptions = [preprocess_text(desc) for desc in job_descriptions]\n",
    "  predictions = best_model.predict(processed_descriptions)\n",
    "  reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "  return [reverse_label_mapping[prediction] for prediction in predictions]  \n",
    "\n",
    "# Example usage\n",
    "job_descriptions = [\n",
    "\"To provide of an effective Joinery resource to ensure the University fabric is efficiently maintained on a day-to-day basis including undertaking Project works. To ensure the effective interaction of Estate and Facilities services with other services\"\n",
    "]\n",
    "predicted_responses = predict_responses(job_descriptions)\n",
    "for job_desc, response in zip(job_descriptions, predicted_responses):\n",
    "  print(f'Job Description: {job_desc}\\nPredicted Response: {response}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
